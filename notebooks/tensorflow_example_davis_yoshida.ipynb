{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "hidden_units = 900\n",
    "n_layers = 5\n",
    "logdir = '/tmp/tf_demo_logs' # Be careful changing this since this directory will be purged when this notebook is run\n",
    "num_examples = 1000\n",
    "noise_scale = 0.1\n",
    "num_epochs = 15\n",
    "learning_rate = 0.0001\n",
    "init_scale = 6 / (hidden_units ** 0.5)\n",
    "nonlinearity = tf.nn.relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean out old model so changes to definition won't mismatch\n",
    "if os.path.exists(logdir):\n",
    "    shutil.rmtree(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate some noisy training data\n",
    "x_data = np.random.uniform(-1, 1, size=(num_examples, 2))\n",
    "label_data = np.array([int(y > np.sin(np.pi * x) + np.random.normal(scale=noise_scale)) for x, y in x_data]) # + np.random.normal(scale=noise_scale, size=num_examples)\n",
    "\n",
    "def plot(points, labels):\n",
    "    colors = ['red', 'blue']\n",
    "    plt.clf()\n",
    "    plt.scatter(*zip(*points), color=[colors[l] for l in labels])           \n",
    "\n",
    "plot(x_data, label_data)\n",
    "plt.show()\n",
    "\n",
    "# Grid of points for evaluating model\n",
    "holdout_x = list(itertools.product(np.arange(-1, 1, 0.03), np.arange(-1, 1, 0.03)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\"Build the model. No actual computation is done here, we're just defining the structure of future computation.\"\"\"\n",
    "    # These variables are used to feed data into the graph\n",
    "    x = tf.placeholder(name='x', shape=(None, 2), dtype=tf.float32)\n",
    "    labels = tf.placeholder(name='labels', shape=None, dtype=tf.int32)\n",
    "    \n",
    "    # Create a list of layer sizes for our network\n",
    "    layer_sizes = [2] + [hidden_units] * (n_layers - 1)\n",
    "    \n",
    "    # For each layer except the last define an affine transformation followed by a nonlinearity\n",
    "    layer_output = x\n",
    "    for i, (in_size, out_size) in enumerate(zip(layer_sizes, layer_sizes[1:])):\n",
    "        with tf.variable_scope('layer' + str(i), reuse=None):\n",
    "            W = tf.get_variable(name='W' + str(i), shape=(in_size, out_size), dtype=tf.float32)\n",
    "            b = tf.get_variable(name='b' + str(i), shape=out_size, dtype=tf.float32)\n",
    "            layer_output = nonlinearity(tf.matmul(layer_output, W) + b)\n",
    "        \n",
    "    global softmax_W\n",
    "    # Compute two logits for a softmax layer\n",
    "    with tf.variable_scope('softmax', reuse=None):\n",
    "        softmax_W = tf.get_variable(name='softmax_W', shape=(hidden_units, 2), dtype=tf.float32)\n",
    "        softmax_b = tf.get_variable(name='softmax_b', shape=2, dtype=tf.float32)\n",
    "        logits = tf.matmul(layer_output, softmax_W) + softmax_b\n",
    "    \n",
    "    # Instead of explicitly computing softmax, just pass logits to this loss functions\n",
    "    # It will one loss per example so take the mean over the batch\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels))\n",
    "    \n",
    "    # Predictions are not used during training but are used for evaluation\n",
    "    predictions = tf.argmax(logits, 1)\n",
    "\n",
    "    # This node is used to apply gradient information to modify variables\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    \n",
    "    \n",
    "    return x, labels, predictions, loss, train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# All variables defined will be put into the default graph\n",
    "with tf.Graph().as_default():\n",
    "    # Set a default name prefix and initializer for all variables\n",
    "    with tf.variable_scope('model', reuse=None, initializer=tf.random_uniform_initializer(minval=-init_scale, maxval=init_scale)):\n",
    "        x_placeholder, label_placeholder, predictions, loss, train_op = build_model()\n",
    "    \n",
    "    # List of indices which will be shuffled for each epoch\n",
    "    example_order = list(range(num_examples))\n",
    "    \n",
    "    # The supervisor automatically saves variable state, and ensures that variables are initialized/loaded\n",
    "    supervisor = tf.train.Supervisor(logdir=logdir)\n",
    "    with supervisor.managed_session() as session:\n",
    "        # Plot the initial (random) decision boundary\n",
    "        print('Initial decision boundary:')\n",
    "        feed_dict = {x_placeholder: holdout_x}\n",
    "        prediction_vals = session.run(predictions, feed_dict=feed_dict)\n",
    "        plot(holdout_x, prediction_vals)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Shuffle the order of examples\n",
    "            np.random.shuffle(example_order)\n",
    "            steps = 0\n",
    "            total_loss = 0\n",
    "            for i in range(num_examples // batch_size):\n",
    "                # Get the indices for this minibatch\n",
    "                indices = example_order[i * batch_size : (i + 1) * batch_size]\n",
    "                # Fetches defines what values in the graph we want to compute\n",
    "                fetche0s = {'loss': loss, 'train_op': train_op}\n",
    "                # A feed_dict defines the values we will insert into the graph\n",
    "                feed_dict = {x_placeholder: x_data[indices], label_placeholder: label_data[indices]}\n",
    "                # This is where all the actual computation happens\n",
    "                output = session.run(fetches=fetches, feed_dict=feed_dict)\n",
    "                \n",
    "                steps += 1\n",
    "                total_loss += output['loss']\n",
    "            avg_loss = total_loss / steps\n",
    "            print('Epoch: {} Average Loss: {}'.format(epoch, avg_loss))\n",
    "                \n",
    "            before = session.run(softmax_W)\n",
    "            # Plot the predicted curve\n",
    "            feed_dict = {x_placeholder: holdout_x}\n",
    "            prediction_vals = session.run(predictions, feed_dict=feed_dict)\n",
    "            after = session.run(softmax_W)\n",
    "            assert (before == after).all()\n",
    "            plot(holdout_x, prediction_vals)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
